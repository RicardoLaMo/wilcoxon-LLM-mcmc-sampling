{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning with Sampling: Your Base Model is Smarter Than You Think\n",
    "\n",
    "**Paper:** [arXiv:2510.14901](https://arxiv.org/abs/2510.14901)  \n",
    "**Authors:** Aayush Karan and Yilun Du (Harvard)\n",
    "\n",
    "This notebook walks through the key concepts of the \"Reasoning with Sampling\" paper and demonstrates the methodology on the **MATH500** dataset.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The paper demonstrates that **base language models** can achieve reasoning capabilities comparable to fine-tuned models through **pure sampling at inference time**, without any additional training. The key innovation is using **MCMC-based power sampling** to sample from sharpened distributions using the base model's own likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q torch transformers datasets numpy scipy matplotlib pandas tqdm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key Concepts\n",
    "\n",
    "### 1.1 Power Distribution\n",
    "\n",
    "The core idea is to sample from a **power distribution** (also called tempered distribution):\n",
    "\n",
    "$$p_\\beta(y | x) \\propto p(y | x)^\\beta$$\n",
    "\n",
    "where:\n",
    "- $p(y | x)$ is the base model's probability of generating response $y$ given prompt $x$\n",
    "- $\\beta > 1$ is the temperature parameter that **sharpens** the distribution\n",
    "- Higher $\\beta$ concentrates probability mass on high-likelihood sequences\n",
    "\n",
    "### 1.2 Why Not Just Low-Temperature Sampling?\n",
    "\n",
    "Power sampling is **NOT** equivalent to low-temperature next-token sampling because:\n",
    "- **Low-temperature sampling** greedily picks high-probability next tokens\n",
    "- **Power sampling** accounts for future paths, favoring tokens that lead to few but high-likelihood completions\n",
    "- This is exactly the behavior needed for reasoning: narrow, high-confidence chains\n",
    "\n",
    "### 1.3 MCMC Algorithm: Metropolis-Hastings with Random Resampling\n",
    "\n",
    "Since directly sampling from $p_\\beta(y | x)$ is intractable, the paper uses **MCMC** with this procedure:\n",
    "\n",
    "1. Start with an initial completion $y_0$ (can be from the base model)\n",
    "2. For iteration $t$:\n",
    "   - **Proposal**: Pick a random span in $y_t$, regenerate it with the base model → get candidate $y'$\n",
    "   - **Acceptance**: Compute acceptance ratio:\n",
    "     $$\\alpha = \\min\\left(1, \\frac{p(y' | x)^\\beta}{p(y_t | x)^\\beta}\\right) = \\min\\left(1, \\left(\\frac{p(y' | x)}{p(y_t | x)}\\right)^\\beta\\right)$$\n",
    "   - Accept $y_{t+1} = y'$ with probability $\\alpha$, otherwise keep $y_{t+1} = y_t$\n",
    "3. After burn-in, collect samples\n",
    "\n",
    "### 1.4 Key Advantages\n",
    "\n",
    "- **Training-free**: No gradient updates needed\n",
    "- **Dataset-free**: No curated training data required\n",
    "- **Verifier-free**: Works beyond easily verifiable domains\n",
    "- **Performance**: Matches or outperforms RL methods like GRPO on MATH500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MATH500 Dataset\n",
    "\n",
    "The MATH dataset contains challenging mathematical problems across different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load MATH500 dataset (subset of MATH dataset)\n",
    "try:\n",
    "    # Try loading from HuggingFace\n",
    "    dataset = load_dataset(\"lighteval/MATH\", split=\"train\")\n",
    "    # Take first 500 for MATH500\n",
    "    math500 = dataset.select(range(min(500, len(dataset))))\n",
    "    print(f\"Loaded {len(math500)} problems from MATH dataset\")\n",
    "except:\n",
    "    print(\"Could not load MATH dataset from HuggingFace. Creating synthetic examples...\")\n",
    "    # Create synthetic math problems for demonstration\n",
    "    math500 = [\n",
    "        {\n",
    "            \"problem\": \"What is 2 + 2?\",\n",
    "            \"solution\": \"2 + 2 = 4\",\n",
    "            \"answer\": \"4\",\n",
    "            \"subject\": \"arithmetic\",\n",
    "            \"level\": \"Level 1\"\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"Solve for x: 2x + 5 = 13\",\n",
    "            \"solution\": \"2x + 5 = 13\\\\n2x = 8\\\\nx = 4\",\n",
    "            \"answer\": \"4\",\n",
    "            \"subject\": \"algebra\",\n",
    "            \"level\": \"Level 2\"\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"What is the derivative of x^2 + 3x?\",\n",
    "            \"solution\": \"Using power rule: d/dx(x^2 + 3x) = 2x + 3\",\n",
    "            \"answer\": \"2x + 3\",\n",
    "            \"subject\": \"calculus\",\n",
    "            \"level\": \"Level 3\"\n",
    "        }\n",
    "    ]\n",
    "    print(f\"Created {len(math500)} synthetic examples\")\n",
    "\n",
    "# Display sample problem\n",
    "sample_idx = 0\n",
    "sample = math500[sample_idx] if isinstance(math500, list) else math500[sample_idx]\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Problem:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Problem: {sample['problem']}\")\n",
    "print(f\"\\nAnswer: {sample['answer']}\")\n",
    "if 'subject' in sample:\n",
    "    print(f\"Subject: {sample['subject']}\")\n",
    "if 'level' in sample:\n",
    "    print(f\"Level: {sample['level']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement MCMC Power Sampling\n",
    "\n",
    "We'll implement a simplified version of the MCMC algorithm. For a real implementation, you would use a full LLM. Here we'll create a mock implementation that demonstrates the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class SamplingResult:\n",
    "    \"\"\"Container for sampling results\"\"\"\n",
    "    text: str\n",
    "    log_prob: float\n",
    "    tokens: List[str]\n",
    "    token_log_probs: List[float]\n",
    "    \n",
    "    @property\n",
    "    def prob(self) -> float:\n",
    "        \"\"\"Convert log probability to probability\"\"\"\n",
    "        return np.exp(self.log_prob)\n",
    "\n",
    "\n",
    "class MockLLM:\n",
    "    \"\"\"\n",
    "    Mock LLM for demonstration purposes.\n",
    "    In practice, you would use a real model like Qwen2.5-Math-7B.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, temperature: float = 1.0):\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def generate(self, prompt: str, max_tokens: int = 50) -> SamplingResult:\n",
    "        \"\"\"\n",
    "        Generate a completion with log probabilities.\n",
    "        This is a mock implementation - replace with real LLM.\n",
    "        \"\"\"\n",
    "        # Mock generation - in practice, use model.generate()\n",
    "        completions = [\n",
    "            \"Let's solve this step by step. First, we identify the equation.\",\n",
    "            \"To solve this problem, we need to apply the formula.\",\n",
    "            \"The answer is obtained by calculating the value.\"\n",
    "        ]\n",
    "        \n",
    "        # Random selection weighted by temperature\n",
    "        idx = np.random.choice(len(completions))\n",
    "        text = completions[idx]\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Mock log probabilities (normally from model)\n",
    "        token_log_probs = np.random.uniform(-2, -0.5, len(tokens)).tolist()\n",
    "        total_log_prob = sum(token_log_probs)\n",
    "        \n",
    "        return SamplingResult(\n",
    "            text=text,\n",
    "            log_prob=total_log_prob,\n",
    "            tokens=tokens,\n",
    "            token_log_probs=token_log_probs\n",
    "        )\n",
    "    \n",
    "    def compute_log_prob(self, prompt: str, completion: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute log probability of a completion given a prompt.\n",
    "        In practice, use model.compute_log_likelihood()\n",
    "        \"\"\"\n",
    "        # Mock implementation - in practice, run model in eval mode\n",
    "        tokens = completion.split()\n",
    "        # Simulate log probs based on completion quality\n",
    "        base_log_prob = -len(tokens) * np.random.uniform(0.5, 1.5)\n",
    "        return base_log_prob\n",
    "    \n",
    "    def resample_span(self, text: str, start_pos: int, end_pos: int) -> str:\n",
    "        \"\"\"\n",
    "        Resample a span of text.\n",
    "        This is the proposal mechanism in MCMC.\n",
    "        \"\"\"\n",
    "        tokens = text.split()\n",
    "        prefix = \" \".join(tokens[:start_pos])\n",
    "        suffix = \" \".join(tokens[end_pos:])\n",
    "        \n",
    "        # Generate new middle part\n",
    "        new_span_options = [\n",
    "            \"we can determine that\",\n",
    "            \"it follows that\",\n",
    "            \"we observe that\",\n",
    "            \"this gives us\"\n",
    "        ]\n",
    "        new_span = np.random.choice(new_span_options)\n",
    "        \n",
    "        return f\"{prefix} {new_span} {suffix}\".strip()\n",
    "\n",
    "\n",
    "class PowerSampler:\n",
    "    \"\"\"\n",
    "    Implements MCMC Power Sampling for LLM reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: MockLLM, beta: float = 2.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Language model to use\n",
    "            beta: Power parameter (beta > 1 sharpens distribution)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.beta = beta\n",
    "        self.acceptance_history = []\n",
    "        \n",
    "    def acceptance_probability(self, log_prob_current: float, log_prob_proposal: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute Metropolis-Hastings acceptance probability.\n",
    "        \n",
    "        alpha = min(1, (p(y'|x) / p(y|x))^beta)\n",
    "        In log space: log(alpha) = min(0, beta * (log p(y'|x) - log p(y|x)))\n",
    "        \"\"\"\n",
    "        log_ratio = self.beta * (log_prob_proposal - log_prob_current)\n",
    "        return min(1.0, np.exp(log_ratio))\n",
    "    \n",
    "    def sample(self, prompt: str, n_iterations: int = 100, burn_in: int = 20) -> Tuple[List[SamplingResult], List[float]]:\n",
    "        \"\"\"\n",
    "        Run MCMC power sampling.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "            n_iterations: Number of MCMC iterations\n",
    "            burn_in: Number of initial samples to discard\n",
    "            \n",
    "        Returns:\n",
    "            samples: List of samples after burn-in\n",
    "            acceptance_rates: Acceptance rate at each iteration\n",
    "        \"\"\"\n",
    "        # Initialize with base model sample\n",
    "        current = self.model.generate(prompt)\n",
    "        current_log_prob = self.model.compute_log_prob(prompt, current.text)\n",
    "        \n",
    "        samples = []\n",
    "        acceptance_rates = []\n",
    "        n_accepted = 0\n",
    "        \n",
    "        for i in tqdm(range(n_iterations), desc=\"MCMC Sampling\"):\n",
    "            # Proposal: resample random span\n",
    "            tokens = current.text.split()\n",
    "            if len(tokens) > 3:\n",
    "                # Pick random span (at least 2 tokens)\n",
    "                span_start = np.random.randint(0, len(tokens) - 2)\n",
    "                span_end = np.random.randint(span_start + 1, len(tokens))\n",
    "                proposal_text = self.model.resample_span(current.text, span_start, span_end)\n",
    "            else:\n",
    "                # If too short, generate new sample\n",
    "                proposal = self.model.generate(prompt)\n",
    "                proposal_text = proposal.text\n",
    "            \n",
    "            # Compute proposal log probability\n",
    "            proposal_log_prob = self.model.compute_log_prob(prompt, proposal_text)\n",
    "            \n",
    "            # Acceptance step\n",
    "            alpha = self.acceptance_probability(current_log_prob, proposal_log_prob)\n",
    "            \n",
    "            if np.random.random() < alpha:\n",
    "                # Accept proposal\n",
    "                current_text = proposal_text\n",
    "                current_log_prob = proposal_log_prob\n",
    "                n_accepted += 1\n",
    "            # else: keep current (rejection)\n",
    "            \n",
    "            # Track acceptance rate\n",
    "            acceptance_rates.append(n_accepted / (i + 1))\n",
    "            \n",
    "            # Collect sample after burn-in\n",
    "            if i >= burn_in:\n",
    "                sample = SamplingResult(\n",
    "                    text=current_text,\n",
    "                    log_prob=current_log_prob,\n",
    "                    tokens=current_text.split(),\n",
    "                    token_log_probs=[current_log_prob / len(current_text.split())] * len(current_text.split())\n",
    "                )\n",
    "                samples.append(sample)\n",
    "        \n",
    "        self.acceptance_history = acceptance_rates\n",
    "        return samples, acceptance_rates\n",
    "\n",
    "\n",
    "print(\"✓ MCMC Power Sampling implementation complete\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demonstration: Standard Sampling vs MCMC Power Sampling\n",
    "\n",
    "Let's compare standard sampling with MCMC power sampling on a math problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create mock LLM\n",
    "model = MockLLM(temperature=1.0)\n",
    "\n",
    "# Get a sample problem\n",
    "problem = math500[0] if isinstance(math500, list) else math500[0]\n",
    "prompt = f\"Problem: {problem['problem']}\\n\\nSolution:\"\n",
    "\n",
    "print(\"Problem:\")\n",
    "print(problem['problem'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Standard sampling\n",
    "print(\"\\n[1] STANDARD SAMPLING (Multiple independent samples)\")\n",
    "print(\"=\"*80)\n",
    "standard_samples = []\n",
    "for i in range(10):\n",
    "    sample = model.generate(prompt)\n",
    "    log_prob = model.compute_log_prob(prompt, sample.text)\n",
    "    standard_samples.append(SamplingResult(\n",
    "        text=sample.text,\n",
    "        log_prob=log_prob,\n",
    "        tokens=sample.tokens,\n",
    "        token_log_probs=sample.token_log_probs\n",
    "    ))\n",
    "\n",
    "print(f\"Generated {len(standard_samples)} independent samples\")\n",
    "print(f\"Average log-prob: {np.mean([s.log_prob for s in standard_samples]):.3f}\")\n",
    "print(f\"Std log-prob: {np.std([s.log_prob for s in standard_samples]):.3f}\")\n",
    "\n",
    "# MCMC Power sampling\n",
    "print(\"\\n[2] MCMC POWER SAMPLING (β=2.0)\")\n",
    "print(\"=\"*80)\n",
    "power_sampler = PowerSampler(model, beta=2.0)\n",
    "mcmc_samples, acceptance_rates = power_sampler.sample(\n",
    "    prompt, \n",
    "    n_iterations=100, \n",
    "    burn_in=20\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(mcmc_samples)} MCMC samples (after burn-in)\")\n",
    "print(f\"Average log-prob: {np.mean([s.log_prob for s in mcmc_samples]):.3f}\")\n",
    "print(f\"Std log-prob: {np.std([s.log_prob for s in mcmc_samples]):.3f}\")\n",
    "print(f\"Final acceptance rate: {acceptance_rates[-1]:.2%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize MCMC Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Log probability distribution comparison\n",
    "ax = axes[0, 0]\n",
    "standard_log_probs = [s.log_prob for s in standard_samples]\n",
    "mcmc_log_probs = [s.log_prob for s in mcmc_samples]\n",
    "\n",
    "ax.hist(standard_log_probs, bins=15, alpha=0.6, label='Standard Sampling', color='blue')\n",
    "ax.hist(mcmc_log_probs, bins=15, alpha=0.6, label='MCMC Power Sampling (β=2.0)', color='red')\n",
    "ax.axvline(np.mean(standard_log_probs), color='blue', linestyle='--', linewidth=2, label='Standard Mean')\n",
    "ax.axvline(np.mean(mcmc_log_probs), color='red', linestyle='--', linewidth=2, label='MCMC Mean')\n",
    "ax.set_xlabel('Log Probability')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Log Probabilities')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Acceptance rate over iterations\n",
    "ax = axes[0, 1]\n",
    "ax.plot(acceptance_rates, linewidth=2, color='green')\n",
    "ax.axhline(acceptance_rates[-1], color='red', linestyle='--', \n",
    "           label=f'Final: {acceptance_rates[-1]:.2%}')\n",
    "ax.axvline(20, color='gray', linestyle=':', label='Burn-in end')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Acceptance Rate')\n",
    "ax.set_title('MCMC Acceptance Rate Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Log prob trace (MCMC chain)\n",
    "ax = axes[1, 0]\n",
    "ax.plot(range(len(mcmc_samples)), mcmc_log_probs, linewidth=1, alpha=0.7, color='purple')\n",
    "ax.axhline(np.mean(mcmc_log_probs), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(mcmc_log_probs):.2f}')\n",
    "ax.set_xlabel('Sample Index (post burn-in)')\n",
    "ax.set_ylabel('Log Probability')\n",
    "ax.set_title('MCMC Chain Trace Plot')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Box plot comparison\n",
    "ax = axes[1, 1]\n",
    "data_to_plot = [standard_log_probs, mcmc_log_probs]\n",
    "bp = ax.boxplot(data_to_plot, labels=['Standard', 'MCMC (β=2.0)'], \n",
    "                patch_artist=True, widths=0.6)\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "ax.set_ylabel('Log Probability')\n",
    "ax.set_title('Log Probability Comparison')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mcmc_sampling_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization saved as 'mcmc_sampling_analysis.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Effect of Beta Parameter\n",
    "\n",
    "Let's explore how different values of β affect the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test different beta values\n",
    "beta_values = [1.0, 1.5, 2.0, 3.0, 5.0]\n",
    "results_by_beta = {}\n",
    "\n",
    "print(\"Testing different β values...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for beta in beta_values:\n",
    "    sampler = PowerSampler(model, beta=beta)\n",
    "    samples, acc_rates = sampler.sample(prompt, n_iterations=100, burn_in=20)\n",
    "    \n",
    "    results_by_beta[beta] = {\n",
    "        'samples': samples,\n",
    "        'log_probs': [s.log_prob for s in samples],\n",
    "        'acceptance_rate': acc_rates[-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"β={beta:.1f}: Mean log-prob={np.mean(results_by_beta[beta]['log_probs']):.3f}, \"\n",
    "          f\"Acceptance rate={acc_rates[-1]:.2%}\")\n",
    "\n",
    "# Visualize effect of beta\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Mean log-prob vs beta\n",
    "ax = axes[0]\n",
    "means = [np.mean(results_by_beta[b]['log_probs']) for b in beta_values]\n",
    "stds = [np.std(results_by_beta[b]['log_probs']) for b in beta_values]\n",
    "ax.errorbar(beta_values, means, yerr=stds, marker='o', markersize=8, \n",
    "            linewidth=2, capsize=5, capthick=2)\n",
    "ax.set_xlabel('β (Power Parameter)', fontsize=12)\n",
    "ax.set_ylabel('Mean Log Probability', fontsize=12)\n",
    "ax.set_title('Effect of β on Sample Quality', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Acceptance rate vs beta\n",
    "ax = axes[1]\n",
    "acc_rates = [results_by_beta[b]['acceptance_rate'] for b in beta_values]\n",
    "ax.plot(beta_values, acc_rates, marker='s', markersize=8, linewidth=2, color='green')\n",
    "ax.set_xlabel('β (Power Parameter)', fontsize=12)\n",
    "ax.set_ylabel('Acceptance Rate', fontsize=12)\n",
    "ax.set_title('Effect of β on MCMC Acceptance', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('beta_parameter_effect.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Beta parameter analysis saved as 'beta_parameter_effect.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wilcoxon Signed-Rank Test: MCMC vs Standard Sampling\n",
    "\n",
    "Now we apply the **Wilcoxon signed-rank test** to statistically compare log-probabilities between MCMC power sampling and standard sampling.\n",
    "\n",
    "### What is the Wilcoxon Signed-Rank Test?\n",
    "\n",
    "- **Non-parametric test** for paired samples (doesn't assume normal distribution)\n",
    "- Tests whether the median difference between paired observations is zero\n",
    "- **Null hypothesis** (H₀): The two sampling methods produce the same log-probability distribution\n",
    "- **Alternative hypothesis** (H₁): MCMC produces different (typically higher) log-probabilities\n",
    "\n",
    "### Why Use It Here?\n",
    "\n",
    "- We want to test if MCMC power sampling **significantly** improves log-probabilities\n",
    "- The test is robust to outliers and doesn't assume normality\n",
    "- Perfect for comparing two sampling strategies on the same problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run comparison on multiple problems\n",
    "n_problems = min(20, len(math500))  # Test on 20 problems\n",
    "n_samples_per_method = 10  # 10 samples per method per problem\n",
    "\n",
    "print(f\"Running comparison on {n_problems} problems...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for i in tqdm(range(n_problems), desc=\"Processing problems\"):\n",
    "    problem = math500[i] if isinstance(math500, list) else math500[i]\n",
    "    prompt = f\"Problem: {problem['problem']}\\n\\nSolution:\"\n",
    "    \n",
    "    # Standard sampling\n",
    "    standard_log_probs = []\n",
    "    for _ in range(n_samples_per_method):\n",
    "        sample = model.generate(prompt)\n",
    "        log_prob = model.compute_log_prob(prompt, sample.text)\n",
    "        standard_log_probs.append(log_prob)\n",
    "    \n",
    "    # MCMC power sampling (β=2.0)\n",
    "    sampler = PowerSampler(model, beta=2.0)\n",
    "    mcmc_samples_result, _ = sampler.sample(prompt, n_iterations=50, burn_in=10)\n",
    "    # Take first n_samples_per_method\n",
    "    mcmc_log_probs = [s.log_prob for s in mcmc_samples_result[:n_samples_per_method]]\n",
    "    \n",
    "    # Store results\n",
    "    comparison_data.append({\n",
    "        'problem_id': i,\n",
    "        'problem': problem['problem'][:50] + '...',  # truncate for display\n",
    "        'standard_mean': np.mean(standard_log_probs),\n",
    "        'mcmc_mean': np.mean(mcmc_log_probs),\n",
    "        'standard_max': np.max(standard_log_probs),\n",
    "        'mcmc_max': np.max(mcmc_log_probs),\n",
    "        'standard_log_probs': standard_log_probs,\n",
    "        'mcmc_log_probs': mcmc_log_probs\n",
    "    })\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'problem_id': d['problem_id'],\n",
    "        'problem': d['problem'],\n",
    "        'standard_mean': d['standard_mean'],\n",
    "        'mcmc_mean': d['mcmc_mean'],\n",
    "        'difference': d['mcmc_mean'] - d['standard_mean']\n",
    "    }\n",
    "    for d in comparison_data\n",
    "])\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df_comparison[['standard_mean', 'mcmc_mean', 'difference']].describe())\n",
    "print(f\"\\nProblems where MCMC > Standard: {(df_comparison['difference'] > 0).sum()} / {n_problems}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform Wilcoxon Signed-Rank Test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WILCOXON SIGNED-RANK TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract paired samples (mean log-prob per problem)\n",
    "standard_means = df_comparison['standard_mean'].values\n",
    "mcmc_means = df_comparison['mcmc_mean'].values\n",
    "\n",
    "# Perform the test\n",
    "statistic, p_value = stats.wilcoxon(standard_means, mcmc_means, alternative='less')\n",
    "# 'less' means we test if standard < mcmc (i.e., mcmc is better)\n",
    "\n",
    "print(f\"\\nNull Hypothesis (H₀): MCMC and Standard sampling produce the same log-probabilities\")\n",
    "print(f\"Alternative Hypothesis (H₁): MCMC produces higher log-probabilities\")\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Wilcoxon statistic: {statistic:.4f}\")\n",
    "print(f\"  p-value: {p_value:.6f}\")\n",
    "print(f\"  Significance level (α): 0.05\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n✓ RESULT: REJECT H₀ (p={p_value:.6f} < 0.05)\")\n",
    "    print(f\"  → MCMC power sampling produces SIGNIFICANTLY higher log-probabilities!\")\n",
    "else:\n",
    "    print(f\"\\n✗ RESULT: FAIL TO REJECT H₀ (p={p_value:.6f} ≥ 0.05)\")\n",
    "    print(f\"  → No significant difference detected\")\n",
    "\n",
    "# Effect size (median of differences)\n",
    "median_diff = np.median(df_comparison['difference'])\n",
    "mean_diff = np.mean(df_comparison['difference'])\n",
    "print(f\"\\nEffect Size:\")\n",
    "print(f\"  Median difference: {median_diff:.4f}\")\n",
    "print(f\"  Mean difference: {mean_diff:.4f}\")\n",
    "print(f\"  Mean improvement: {(mean_diff / np.abs(np.mean(standard_means)) * 100):.2f}%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Wilcoxon Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Paired comparison\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, df_comparison['standard_mean'], width, label='Standard', alpha=0.8, color='blue')\n",
    "ax.bar(x + width/2, df_comparison['mcmc_mean'], width, label='MCMC (β=2.0)', alpha=0.8, color='red')\n",
    "ax.set_xlabel('Problem ID')\n",
    "ax.set_ylabel('Mean Log Probability')\n",
    "ax.set_title('Paired Comparison: Standard vs MCMC')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Difference distribution\n",
    "ax = axes[0, 1]\n",
    "differences = df_comparison['difference'].values\n",
    "ax.hist(differences, bins=15, alpha=0.7, color='green', edgecolor='black')\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='No difference')\n",
    "ax.axvline(np.median(differences), color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'Median: {np.median(differences):.3f}')\n",
    "ax.set_xlabel('Difference (MCMC - Standard)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Log-Prob Differences')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Scatter plot with regression\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(df_comparison['standard_mean'], df_comparison['mcmc_mean'], \n",
    "           alpha=0.6, s=100, color='purple', edgecolors='black', linewidth=1)\n",
    "# Add diagonal line (y=x)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "]\n",
    "ax.plot(lims, lims, 'k--', alpha=0.5, linewidth=2, label='y=x (no difference)')\n",
    "ax.set_xlabel('Standard Sampling (Mean Log-Prob)')\n",
    "ax.set_ylabel('MCMC Sampling (Mean Log-Prob)')\n",
    "ax.set_title('Standard vs MCMC: Scatter Plot')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Statistical summary\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "# Create text summary\n",
    "summary_text = f\"\"\"\n",
    "WILCOXON SIGNED-RANK TEST RESULTS\n",
    "{'='*50}\n",
    "\n",
    "Sample Size: {len(df_comparison)} problems\n",
    "Samples per method: {n_samples_per_method}\n",
    "\n",
    "Test Statistics:\n",
    "  • Wilcoxon statistic: {statistic:.4f}\n",
    "  • p-value: {p_value:.6f}\n",
    "  • Significance level: α = 0.05\n",
    "\n",
    "Effect Size:\n",
    "  • Mean difference: {mean_diff:.4f}\n",
    "  • Median difference: {median_diff:.4f}\n",
    "  • Std of differences: {np.std(differences):.4f}\n",
    "\n",
    "Conclusion:\n",
    "\"\"\"\n",
    "\n",
    "if p_value < 0.05:\n",
    "    summary_text += f\"  ✓ MCMC is SIGNIFICANTLY better\\n\"\n",
    "    summary_text += f\"    (p = {p_value:.6f} < 0.05)\\n\\n\"\n",
    "    summary_text += f\"  MCMC power sampling achieves\\n\"\n",
    "    summary_text += f\"  {(mean_diff / np.abs(np.mean(standard_means)) * 100):.2f}% improvement\\n\"\n",
    "    summary_text += f\"  in log-probability\"\n",
    "else:\n",
    "    summary_text += f\"  ✗ No significant difference\\n\"\n",
    "    summary_text += f\"    (p = {p_value:.6f} ≥ 0.05)\"\n",
    "\n",
    "ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, \n",
    "        fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wilcoxon_test_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Wilcoxon test visualization saved as 'wilcoxon_test_results.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional Analysis: Pass@k Performance\n",
    "\n",
    "One key metric in the paper is **Pass@k** - the probability that at least one of k samples is correct.\n",
    "\n",
    "For MCMC power sampling, we expect:\n",
    "- Higher quality samples overall (higher log-probs)\n",
    "- Better Pass@k performance (more correct answers in top-k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def estimate_pass_at_k(n_correct: int, n_total: int, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Estimate Pass@k using the formula from the paper.\n",
    "    \n",
    "    Pass@k = E[1 - C(n_total - n_correct, k) / C(n_total, k)]\n",
    "    \n",
    "    where C(n, k) is the binomial coefficient \"n choose k\"\n",
    "    \"\"\"\n",
    "    if n_total < k:\n",
    "        return 0.0\n",
    "    if n_correct >= k:\n",
    "        return 1.0\n",
    "    \n",
    "    from scipy.special import comb\n",
    "    return 1.0 - (comb(n_total - n_correct, k) / comb(n_total, k))\n",
    "\n",
    "\n",
    "# Simulate Pass@k analysis\n",
    "print(\"Pass@k Analysis (Simulated)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: In practice, you would evaluate actual correctness.\")\n",
    "print(\"Here we simulate by assuming higher log-prob → higher chance of correctness\\n\")\n",
    "\n",
    "k_values = [1, 5, 10, 20, 50]\n",
    "n_total = 100  # Total samples\n",
    "\n",
    "# Simulate: assume top 30% of standard and top 50% of MCMC are correct\n",
    "# (MCMC produces better samples)\n",
    "standard_n_correct = int(0.30 * n_total)\n",
    "mcmc_n_correct = int(0.50 * n_total)\n",
    "\n",
    "print(f\"Simulated scenario:\")\n",
    "print(f\"  Standard sampling: {standard_n_correct}/{n_total} correct ({100*standard_n_correct/n_total:.0f}%)\")\n",
    "print(f\"  MCMC sampling: {mcmc_n_correct}/{n_total} correct ({100*mcmc_n_correct/n_total:.0f}%)\\n\")\n",
    "\n",
    "results_passk = []\n",
    "for k in k_values:\n",
    "    standard_passk = estimate_pass_at_k(standard_n_correct, n_total, k)\n",
    "    mcmc_passk = estimate_pass_at_k(mcmc_n_correct, n_total, k)\n",
    "    results_passk.append({\n",
    "        'k': k,\n",
    "        'standard': standard_passk,\n",
    "        'mcmc': mcmc_passk,\n",
    "        'improvement': mcmc_passk - standard_passk\n",
    "    })\n",
    "    print(f\"Pass@{k:2d}:  Standard={standard_passk:.3f}  |  MCMC={mcmc_passk:.3f}  |  \"\n",
    "          f\"Δ={mcmc_passk - standard_passk:+.3f}\")\n",
    "\n",
    "# Visualize Pass@k\n",
    "df_passk = pd.DataFrame(results_passk)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_passk['k'], df_passk['standard'], marker='o', markersize=8, \n",
    "         linewidth=2, label='Standard Sampling', color='blue')\n",
    "plt.plot(df_passk['k'], df_passk['mcmc'], marker='s', markersize=8, \n",
    "         linewidth=2, label='MCMC Power Sampling (β=2.0)', color='red')\n",
    "plt.xlabel('k (number of samples)', fontsize=12)\n",
    "plt.ylabel('Pass@k', fontsize=12)\n",
    "plt.title('Pass@k Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "plt.ylim([0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.savefig('passk_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Pass@k analysis saved as 'passk_analysis.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "### Main Findings from the Paper\n",
    "\n",
    "1. **Power Sampling Works**: MCMC-based power sampling can extract better reasoning from base models without fine-tuning\n",
    "\n",
    "2. **Matches RL Performance**: On MATH500, power sampling achieves results comparable to or better than GRPO (Group Relative Policy Optimization)\n",
    "\n",
    "3. **Training-Free**: No need for:\n",
    "   - Curated datasets\n",
    "   - Reward models or verifiers\n",
    "   - Gradient-based optimization\n",
    "   - Hyperparameter tuning for training stability\n",
    "\n",
    "4. **Generalizes Well**: Works across:\n",
    "   - Multiple model families (Qwen, Phi, etc.)\n",
    "   - Different domains (math, code, science, general reasoning)\n",
    "\n",
    "### Implementation Insights\n",
    "\n",
    "1. **β Parameter**: \n",
    "   - β = 1: Standard sampling (no sharpening)\n",
    "   - β > 1: Power sampling (sharpens toward high-likelihood)\n",
    "   - Typical values: β ∈ [2, 5]\n",
    "   - Trade-off: Higher β → better samples but lower acceptance rate\n",
    "\n",
    "2. **MCMC Design**:\n",
    "   - Random span resampling as proposal mechanism\n",
    "   - Metropolis-Hastings acceptance based on likelihood ratio\n",
    "   - Requires burn-in period (typically 20-50 iterations)\n",
    "\n",
    "3. **Computational Cost**:\n",
    "   - More expensive than standard sampling (multiple model evaluations)\n",
    "   - But cheaper than training with RL\n",
    "   - Can be parallelized across problems\n",
    "\n",
    "### When to Use Power Sampling\n",
    "\n",
    "✓ **Use when:**\n",
    "- You have a base model but no training data/compute for fine-tuning\n",
    "- You need better reasoning on complex tasks (math, code, logic)\n",
    "- You can afford extra inference-time computation\n",
    "- You want diverse high-quality samples (Pass@k scenarios)\n",
    "\n",
    "✗ **Don't use when:**\n",
    "- Simple tasks where standard sampling suffices\n",
    "- Extreme latency constraints\n",
    "- You already have a well-tuned RL model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To use this in practice:\n",
    "1. Clone the repository: `https://github.com/aakaran/reasoning-with-sampling`\n",
    "2. Install dependencies and download a base model (e.g., Qwen2.5-Math-7B)\n",
    "3. Run power sampling with different β values\n",
    "4. Evaluate with your domain-specific metrics\n",
    "5. Compare against your baseline (fine-tuned model or standard sampling)\n",
    "\n",
    "**Paper citation:**\n",
    "```\n",
    "Karan, A., & Du, Y. (2024). Reasoning with Sampling: Your Base Model is \n",
    "Smarter Than You Think. arXiv preprint arXiv:2510.14901.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save comparison results to CSV\n",
    "df_comparison.to_csv('mcmc_vs_standard_comparison.csv', index=False)\n",
    "print(\"✓ Comparison data saved to 'mcmc_vs_standard_comparison.csv'\")\n",
    "\n",
    "# Save Pass@k results\n",
    "df_passk.to_csv('passk_results.csv', index=False)\n",
    "print(\"✓ Pass@k results saved to 'passk_results.csv'\")\n",
    "\n",
    "# Save statistical test results\n",
    "test_results = {\n",
    "    'test': 'Wilcoxon Signed-Rank Test',\n",
    "    'statistic': statistic,\n",
    "    'p_value': p_value,\n",
    "    'alpha': 0.05,\n",
    "    'significant': p_value < 0.05,\n",
    "    'mean_difference': mean_diff,\n",
    "    'median_difference': median_diff,\n",
    "    'n_problems': len(df_comparison),\n",
    "    'n_samples_per_method': n_samples_per_method\n",
    "}\n",
    "\n",
    "with open('wilcoxon_test_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "print(\"✓ Statistical test results saved to 'wilcoxon_test_results.json'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. mcmc_sampling_analysis.png - MCMC convergence visualization\")\n",
    "print(\"  2. beta_parameter_effect.png - Effect of β parameter\")\n",
    "print(\"  3. wilcoxon_test_results.png - Statistical comparison\")\n",
    "print(\"  4. passk_analysis.png - Pass@k performance\")\n",
    "print(\"  5. mcmc_vs_standard_comparison.csv - Raw comparison data\")\n",
    "print(\"  6. passk_results.csv - Pass@k data\")\n",
    "print(\"  7. wilcoxon_test_results.json - Statistical test summary\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
